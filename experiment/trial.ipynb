{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key= groq_api_key,\n",
    "               model=\"mixtral-8x7b-32768\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Generate a simple one Random question related to the topic {topic} in dictionary format. Each dictionary should contain two keys:\n",
    "\n",
    "\"question\": A question text related to the topic.\n",
    "\"answer\": A one-word or one-liner answer to the question.\n",
    "\n",
    "Ensure that the question covers various aspects of the topic. Example topics to include might vary based on the subject but should cover a wide range of related subtopics.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# Generate a simple one question related to the topic {topic} in dictionary format.\n",
    "\n",
    "\n",
    "#     \"question\": \"<Your Question Here>\",\n",
    "#     \"answer\": \"<Your Answer Here>\"\n",
    "\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dictOutput(BaseOutputParser):\n",
    "    def parse(self, text: str) -> Dict[str, str]:\n",
    "        # Extracting the JSON part from the text\n",
    "        import re\n",
    "        match = re.search(r'\\{[^}]+\\}', text)\n",
    "        if match:\n",
    "            json_text = match.group(0)\n",
    "\n",
    "            # Load the JSON data\n",
    "            import json\n",
    "            data = json.loads(json_text)\n",
    "\n",
    "            # Extract the question and answer\n",
    "            question = data[\"question\"]\n",
    "            answer = data[\"answer\"]\n",
    "\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"No JSON object found in the input text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "chain = prompt | llm | dictOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"topic\":\"sports\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is the maximum number of sets in a men's singles tennis match?\",\n",
       " 'answer': 'Five sets'}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a common cause of iron deficiency in humans?\n",
      "Anemia\n"
     ]
    }
   ],
   "source": [
    "print(response[\"question\"])\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Incorrect:  The answer is incorrect or unrelated to the reference\n",
    "Correct:  The answer is correct and aligns with the reference, including minor misspellings.\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\n",
    "    \"labeled_score_string\",\n",
    "    criteria=accuracy_criteria,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ground truth: 2\n"
     ]
    }
   ],
   "source": [
    "# evaluator = load_evaluator(\"labeled_criteria\", llm=llm, criteria=\"correctness\")\n",
    "\n",
    "# We can even override the model's learned knowledge using ground truth labels\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=response[\"question\"],\n",
    "    prediction= \"Anemia\",\n",
    "    reference=response[\"answer\"],\n",
    ")\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect\n"
     ]
    }
   ],
   "source": [
    "if eval_result[\"score\"]>6:\n",
    "    print(\"correct\")\n",
    "else:\n",
    "    print(\"incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation:\n",
      "The assistant's response is incomplete and not accurate. Anemia is a condition that results from a lack of red blood cells or insufficient hemoglobin, which can be caused by an iron deficiency. However, the assistant's response does not specify that iron deficiency is a cause of anemia, nor does it explain what iron deficiency is or its relationship to anemia.\n",
      "\n",
      "Rating: [[2]]\n",
      "\n",
      "The assistant's response is not entirely incorrect, but it is incomplete and lacks sufficient detail to be considered accurate. The assistant could have provided a more comprehensive answer by explaining that iron deficiency is a common cause of anemia, and then elaborating on the reasons for iron deficiency.\n"
     ]
    }
   ],
   "source": [
    "print(eval_result[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = \"\"\"\n",
    "You are an expert evaluator. Below, you will find a reference question with its correct answer, as well as the user's answer. Your task is to evaluate the user's answer based on the provided criteria.\n",
    "\n",
    "Reference Question and Answer:\n",
    "Question and answer: {qa}\n",
    "\n",
    "User's Answer:\n",
    "{userAnswer}\n",
    "\n",
    "Evaluation Criteria:\n",
    "Incorrect: The answer is incorrect or unrelated to the reference.\n",
    "Correct: The answer is correct and aligns with the reference, including minor misspellings.\n",
    "\n",
    "Please provide your evaluation based on the criteria above in the following format:\n",
    "Result: [Correct/Incorrect]\n",
    "Reason: [Explanation for your evaluation in short]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"qa\",\"userAnswer\"],\n",
    "    template=prompt_template1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = chain2.invoke({\"qa\":response,\"userAnswer\":\"5\"},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Correct\n",
      "Reason: The user's answer of \"5\" matches the correct answer of \"Five sets\", as it refers to the same quantity. The missing word \"sets\" does not affect the accuracy of the answer.\n"
     ]
    }
   ],
   "source": [
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
